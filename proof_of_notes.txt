.Type of SQL Projects:

1. Data Warehousing: "Organize, Structure, Prepare"
+ETL/ ELT Processing
+Data Architecture
+Data Integration
+Data Cleansing
+Data Load
+Data Modeling

2. Exploratory Data Analysis (EDA) : "Understand Data"
+Basic Queries
+ Data Profiling
+ SImple Aggregations
+ Subquery

3. Advanced Data Analytics: "Answer Business Questions"
+ Complex Queries
+ Window Functions
+ CTE
+ Sunqueries
+ Reports

1. Data Warehouse
A subject-oriented, integrated, time-variant, and non-volatile collection of data in support of management's decision-making process.

-> Moving data from SOURCE to TARGET in reality is not a straightforward process, usually you need to apply multiple different variants of ETL when moving between layers in a data architecture between source and target depending on your needs.
-> Moving from Source into a Target through a data architecture with multiple layers. For example:
Source -> Layer1 : use EL because you want to keep the data intact.
Layer1 -> Layer2 : use ETL to further extract and transform the data as per requirements
Layer2 -> Layer3 : use TL only to transform the whole data in Layer2 and load it into the Layer3
Layer3 -> Layer4: use only L because you only want to duplicate the data. Then use T inside Layer4 to transform the data further
-------------------------------
. Extraction methods
+ Pull Extraction or Push Extraction: the code pulling from the source, or the source is pushing the data
. Extraction Types
+ Full Extraction or Incremental Extraction: either everyday we pull all the data from the warehouse, or everyday we only identify new data and extract it
. Extract Techniques
+ Manual Data Extraction: someone physically accesses a source system and extracts data manually.
+ Database Querying: connecting to a database and using a query to extract the data.
+ File Parsing: data is extracted by parsing a file to the data warehouse.
+ API Calls: Writing code to connect to an API and extract data.
+ Event Based Streaming: (e.g., Kafka) data is extracted as streaming events.
+ CDC: (Change Data Capture) similar to streaming, CDC captures and delivers changes made to a database in real-time or near-real-time.
+ Web Scraping: A code runs to extract information directly from the web.
-------------------------------
. Transformation Techniques
+ Data Enrichment: Adding new, relevant information to datasets to enhance their value. This can involve combining data from multiple sources or deriving new attributes.
+ Data Integration: Merging data from multiple source systems into a single, unified data model or dataset. This is particularly important for providing a holistic view from disparate sources
+ Derived Columns: Creating new columns based on calculations or transformations of existing ones (e.g., splitting a product key into category ID and product number).
+ Data Normalization and Standardization: Mapping coded or abbreviated values to more meaningful, user-friendly descriptions (e.g., converting 'F' to 'Female' or 'D' to 'Germany').
+ Business Rules and Logic: Applying specific business criteria and formulas to the data (e.g., ensuring sales = quantity * price or defining how to handle historical data).
+ Data Aggregations: Summarizing data to a different granularity (e.g., summing sales by month or product category).

+ Data Cleansing: this is a critical process to fix data quality issues and ensure consistency. It includes:
▪ Removing Duplicates: Identifying and retaining only the most relevant record for each primary key to ensure uniqueness.
Data Filtering: Selecting specific data based on criteria, often used in conjunction with removing duplicates.
▪ Handling Missing Data: Filling blanks or nulls by adding default values (e.g., replacing nulls with 'Not Available' or 'Unknown', or zero for numerical fields).
▪ Handling Invalid Values/Data: Correcting or removing data that does not conform to expected patterns or ranges (e.g., future birthdates, negative costs, or incorrect numerical representations of dates).
▪ Removing Unwanted Spaces/Trimming: Eliminating leading or trailing spaces from string values to ensure consistency.
▪ Detecting Outliers: Identifying and managing data points that significantly deviate from the majority.
▪ Casting Data Types: Converting data from one type to another (e.g., from integer to date, or specifying NVARCHAR lengths)
-------------------------------

. Processing Types: How frequently and in what manner data is processed for loading into the data warehouse.
- Batch Processing: Loading the data warehouse in one large batch of data. Typically run as a one-time job designed to refresh the entire data warehouse and its associated reports. Usually Scheduled, e.g twice a day.
- Stream Processing: In contrast, aim to process data as soon as a change occurs in the source system. The primary goal is to achieve a real-time data warehouse, meaning data is proccessed through all layers as soon as it changes at the source <- much more challenging to implement in data warehousing projects.

. Load Methods: Refer to the specific ways in which data is inserted or update within the data warehouse tables, often applied within the context of a chosen processing type.
- Full Load: All records from the tables are loaded every day. Involve refreshing the entire content of a table by first emptying it and then inserting all the data from the source.
+ Truncate & Insert: involve making the table completely empty using a TRUNCATE TABLE command and then inserting all the data from scratch. For efficient loading of large amounts of data from files (like CSVs) into the database in one operation, rather than row by row, the BULK INSERT statement is employed.
+ Upert: update insert, to update existing records and inserts new ones.
+ Drop, Create, Insert: involve dropping the entire table, recreating it from scratch, and then inserting the data. It is very similar to TRUNCATE but also removes and redefines the table structure.
- Incremental Load: is a smarter approach where only the new or changed data is identified, extracted, and then loaded. This method avoids loading the entire dataset daily.
+ Upsert: update and insert. Similar to full load upsert, involve updating existing records and inserting new ones into the tables.
+ Append: only insert. For sources that behave like logs, data can simply be appended to the table without needing updates.
+ Merge: update, insert and delete. Kind of similar to upsert but also includes the functionaility to delete records.

. Slowly Changing Dimensions (SCD - How to handle historical data):
+ SCD Type 0 (SCD0 - No Historization): means nothing should be changed at all once data enters the data warehouse. Records are not updated.
+ SCD Type 1 (SCD1 - Overwrite): involve overwriting existing records with new info from the source system. While it updates the records, it means that the historical data is lost because the old values are replaced by the new ones.
+ SCD Type 2 (SCD2 - Add Historization): add historization to a table. For each change received from the source system, new records are inserted. The old data is not overwritten or deleted, instead it is marked as inactive, and the newly inserted record becomes the active one, preserving a complete history of changes over time.

-------------------------------
Data Architectures options:
. Data Warehouse
. Data Lake
. Data Lakehouse (most modern approach)
. Data Mesh (decentralized data structure)

Data Warehouse:
+First type of approach: Inmon
(Source) -> Stage -> EDW (enterprised data warehouse - using 3NF) -> Data Marts -> (Reporting or Analysis)

+Second type of approach: Kimbaff
(Source) -> Stage -> (skipped EDW) Data Marts -> (Reporting or Analysis)

+Third type of approach: Data Vault
(Source) -> Stage -> (splitted EDW) Raw Vault -> Business Vault -> Data Marts -> (Reporting or Analysis)

+Fourth type of approach: Medallion Architecture
(Source) -> Bronze -> Silver -> Gold -> (Reporting or Analysis)

Medallion Architecture approach:
+Bronze Layer: (def) Raw, unprocessed data as-is from sources. (obj) Traceability & Debugging. (obj type) Tables. (Load Method) Truncate & Insert. (Data Transformation) None (as-is). (Data Modelling) None (as-is). (Target Audience) Data Engineers.
+Silver Layer: (def) Clean & standardrized data . (obj) (Intermediate Layer) Prepare Data for Analysis. (obj type) Tables. (Load Method) Truncate & Insert. (Data Transformation) Data Cleaning, Data Standardrization, Data Normalization, Derived Columns, Data Enrichment. (Data Modelling) None (as-is). (Target Audience) Data Analysists, Data Engineers.
+Gold Layer: (def) Business-Ready data. (obj) Provide data to be consumed for reporting & Analytics. (obj type) Views. (Load Method)
None. (Data Transformation) Data Integration, Data Aggregation, Business Logic & Rules. (Data Modelling) Star Schema, Aggregated objects, Flat Tables. (Target Audience) Data Analysists, Business Users.

Mindset for Data Engineers: Separation of Concern

---
Naming Conventions: Set of Rules or Guidelines for naming anything in the project (Database, Schema, Tables, Store Procedures)
1 SCREAMING_SNAKE_CASE
2 kebab-case
3 camelCase
4 snake_case

-> For Table naming in SQL database, follow these principles: use snake_case naming convention, use English for all names, do not use SQL reserved words as object names (e.g not naming a table "Table")
--> Bronze rules:
. All names must start with the source system name, and table names must match their original names without renaming.
. <sourcesystem>_<entity> (e.g crm_customer_info - customer information from the CRM system)
--> Silver rules:
. All names must start with the source system name, and table names must match their original names without renaming.
. <sourcesystem>_<entity> (e.g crm_customer_info - customer information from the CRM system)
--> Gold rules:
. All names must use meaningful, business-aligned names for tables, starting with the category prefix.
. <category>_<entity> (e.g dim_customer - dimension table for customer data, fact_sales - fact table containing sales transactions, agg_customers - aggregated table containing customers data)

-> For Column naming in SQL database
--> Surrogate Keys:
. All primary keys in dimension tables must use the suffix "_key".
. <table_name>_key (e.g customer_key - surrogate key in the dim_customer table)
--> Technical Columns:
. All technical columns must start with the prefix "dwh_" which is exclusively for system-generated metadata, followed by a descriptive name indicating the column's purpose.
. dwh_<column_name> (e.g dwh_load_data - system-generated column used to store the data when the record was loaded)

-> For Stored Procedure naming in SQL database
. All stored procedures used for loading data must follow the naming pattern:
. load_<layer> (e.g load_silver - stored procedure for loading data into the Silver layer)

-----------
Create Database 'DataWarehouse'
"GO": separate batches when working with multiple SQL statements.
"IF EXISTS (SELECT 1 FROM sys.databases WHERE name = 'DataWarehouse')
BEGIN
    ALTER DATABASE DataWarehouse SET SINGLE_USER WITH ROLLBACK IMMEDIATE;
    DROP DATABASE DataWarehouse;"
-> SELECT 1: instead of selecting actual data (like "name" or "database_id"), we SELECT 1 which is a common optimization. We don't care about what is returned, only check for its existence when compared with WHERE name = '...'
-----------
                                        Bronze Layer                           Silver Layer                               Gold Layer
Data Type                     		Raw, unprocessed data                  Clean & standardized data                 Business-Ready data
                                       as-is from sources

Objective                          Traceability & Debugging                  (Intermediate Layer)                Provide data to be consumed
                                                                            Prepare Data for Analysis             for reporting & Analytics

Object Type                                Tables                                  Tables                                    Views
Load Method                               Full Load                               Full Load                                  None
                                      (Truncate & Insert)                     (Truncate & Insert)

                                                                              - Data Cleaning                         - Data Integration
                                                                              - Data Standardization                  - Data Aggregation
Data Transformation                      None (as-is)                         - Derived Columns                       - Business Logic & Rules
                                                                              - Data Enrichment

                                                                                                                          - Star Schema
Data Modeling                            None (as-is)                            None (as-is)                             - Aggregated Objects
                                                                                                                          - Flat Tables

Target Audience                        - Data Engineers                          - Data Analysis                          - Data Analysis
                                                                                 - Data Engineers                         - Business Users

-----------
CREATE BRONZE LAYER
1. Analysing: Source Systems
- Interview Source System Experts
+ Business Context & Ownership
"Who owns the data?"
"What Business Process it supports?": include Customer transactions, Supply chain logistics and Finance reporting. This helps in understanding the importance of your data.
"System & Data documentation": your learning material about the data. This can save a lot of time later when designing new data models.
"Data Model & Data Catalog": a data catalog with descriptions of columns and tables is very helpful for understanding how to join tables in the data warehouse.

+ Architecture & Technology Stack (General Technical questions)
"What is data stored? (SQL Servers, Oracle, AWS, Azure,...)": helps determine the appropriate connection methods and tools needed for extraction.
"How are the integration capabilities? (API, Kafka, File Extract, Direct DB, ...)": focus on how data can be extracted from the source system and dictate the mechanism for data ingestion.

+ Extract & Load (In-depth Technical questions)
"Incremental vs. Full Loads?": determine if the data can be loaded incrementally (only new or changed data) or if a full load (reloading all data each time).
"Data Scope & Historical Needs": clarify if all historical data is required, or only a specific period. It also investigates if historical data is already managed within the source system or if it needs to be built into the data warehouse.
"What is the expected size of the extracts?": understand the volume of data (mbs,gbs,tbs) to select the appropriate tools and platforms for connecting to the source system as well as handle the data volume.
"Are there any data volume limitations?": identify any limitations of older source systems that might struggle with performance when large amounts of data are extracted. 
"How to avoid impacting the source system's performance?"
" Authentication & Authorization (tokens, SSH keys, VPN, IP whitelisting, ...)": how to securely access the data in the source system.

2. Coding: Data Ingestion
"Data Profilling": Explore the data to identify column names and data types.
- Create SQL DDL scripts for ALL CSV files

- Develop SQL Load Scripts
+Full Loads by: use TRUNCATE which can quickly delete all rows from a table, resetting it to an empty state. Then use BULK INSERT.
-> save frequently used SQL code in stored procedures in database.
+ Quality Check: Check that the data has not shifted and is in the correct columns.
-> add Print to track execution, debug issues, and understand its flow
-> SQL runs the TRY block, and if it fails, it runs the CATCH block to handle the error
-> Tracking ETL duration helps to identify bottlenecks, optimize performance, monitor trends, detect issues.

3. Validating: Data Completeness & Schema Checks

4. Document: Draw Data Flow (Draw.io)

-----------
CREATE SILVER LAYER
1. Analysing: Explore & Understand the Data
- Add Metadata columns when create the DDL for Silver Tables
+ create_date: the record's load timestamp.
+ update_date: the record's last update timestamp.
+ source_system: the origin system of the record.
+ file_location: the file source of the record.
- For complex transformations in SQL, we typically narrow it down to a specific example and brainstorm multiple solution approaches
- In data warehouse, we aim to store clear and meaningful values rather than uing abbreviated terms.
- We use the default value 'n/a' for missing values.
- Make sure date data is the correct data type DATE, not STRING or NUM.
- Apply UPPER() just in case mixed-case values appear later in your column.
- Apply TRIM() just in case spaces appear later in your column.

2. Coding: Data Cleansing
- Check Quality of Bronze
- Write Data Transformation
- Insert into Silver

3. Validating: Data Correctness Checks
+ Metadata columns: Extra columns added by data engineers that do not originate from the source data.
+ ROW_NUMBER() : Assigns a unique number to each row in a result set, based on a defined order.
+ TRIM() : Removes leading and trailing spaces from a string
+ SUBSTRING(string, start position-1, end position-n): Extracts a specific part of a string value
+ REPLACE(string, to-be-replaced, replace): replace something
+ LEN(string): returns the number of characters in a string
+ ISNULL(target, desired value): replaces NULL values with a specified replacement value
+ CASE WHEN UPPER(TRIM(key)) = 'W' THEN 'Win'
             WHEN UPPER(TRIM(key)) = '...' THEN '...'
ELSE '...'
END AS key,
-> quick CASE WHEN idealy for simple value mapping:
CASE UPPER(TRIM(key))
          WHEN 'W' THEN 'Win'
          WHEN '...' THEN '...'
ELSE '...'
END AS key,

+ LEAD(next_object) : Access values from the next row within a window without using a self-join
+ LAG(previous_object) : Access data from the previous row within the same result set without using a self-join
+ ISNULL() : Replaces NULL values with a specified replacement value then return the whole data set - simple, single replacement check. ISNULL() can only check for NULL in a single expression and replace it with a single alternative, it doesn't work with multiple fallbacks.
+ COALESCE() : Returns the first non-NULL expression in a list of expressions. More versatile than ISNULL() because it can handle a chain of possible values. For example: COALESCE(home_phone, work_phone, 'No Phone Provided') AS contact_phone
+ CAST (start_date AS DATE) AS start_date : Cast data type

- Quality Check:
*For complex transformations in SQL, we typically narrow it down to a specific example and brainstorm multiple solution and approaches*
+ A Primary Key must be unique and not null
---
SELECT
cst_id,
COUNT(*)
FROM bronze.crm_cust_info
GROUP BY cst_id
HAVING COUNT(*) >1 OR cst_id IS NULL
-> show cst_id with duplicates and also include NULL result (doesn't need to be duplicated)
---
-> focus on the issue which is the duplicate cst_id
SELECT
*
FROM bronze.crm_cust_info
WHERE cst_id = 29466
-> show the duplicated entries of cst_id = 29466
-> Check the latest entry with timestamp because it holds the freshest information
---
-> we have to rank all those values based on the created date and only pick the highest one with ROW_NUMBER()
SELECT
*,
ROW_NUMBER() OVER (PARTITION BY cst_id ORDER BY cst_create_date DESC) as flag_last
FROM bronze.crm_cust_info
WHERE cst_id = 29466
-> we get the duplicate entries with the addition of flag_last row, the number 1 row is the latest entry
---
-> to check everything with the addition of flag_last
SELECT
*,
ROW_NUMBER() OVER (PARTITION BY cst_id ORDER BY cst_create_date DESC) as flag_last
FROM bronze.crm_cust_info
---
-> do a double check
SELECT
*
FROM (
SELECT
*,
ROW_NUMBER() OVER (PARTITION BY cst_id ORDER BY cst_create_date DESC) as flag_last
FROM bronze.crm_cust_info
)t WHERE flag_last != 1
-> now we can see all the data that we don't need with flag_last != 1 because they are causing duplication and also have an old status (older timestamp)
---
-> to find the freshest data from this primary key cst_id = 29466
SELECT
*
FROM (
SELECT
*,
ROW_NUMBER() OVER (PARTITION BY cst_id ORDER BY cst_create_date DESC) as flag_last
FROM bronze.crm_cust_info
)t WHERE flag_last = 1 AND cst_id = 29466

- LOAD & CLEAN process
-- crm_prd_info
+ Check for unwanted spaces in string values
SELECT cst_firstname
FROM bronze.crm_cust_info
WHERE cst_firstname != TRIM(cst_firstname)
-> list of all names where we have spaces

+ (Data Normalization/ Data Standardization)  Check the consistency of values in low cardinality columns
-> maps coded values to meaningful, user-friendly descriptions
-> use DISTINCT to check for low cardinality
SELECT DISTINCT cst_gndr
FROM bronze.crm_cust_info
-> transform
SELECT
cst_gndr,
CASE WHEN cst_gndr = 'F' THEN 'Female'
          WHEN cst_gndr = 'M' THEN 'Male',

+ Handling Missing data
->fills in the blanks by adding a default value
ELSE 'n/a';

+ Remove Duplicates, Data Filtering
-> Ensure only one record per entity by identifying and retaining the most relevant row.

+ Creating Derived columns
-> Create new columns based on calculations or transformations of existing ones.
-> filters out unmatched data after applying transformation

+ Data Enrichment
-> Add new, relevant data to enhance the dataset for analysis

--> In general, these are some of the Data Transformation methods.

SELECT
prd_id,
REPLACE(SUBSTRING(prd_key, 1, 5), '-', '_') AS cat_id,
SUBSTRING(prd_key, 7, LEN(prd_key)) AS prd_key,
prd_nm,
ISNULL(prd_cost, 0) AS prd_cost,
CASE UPPER(TRIM(prd_line))
	WHEN 'M' THEN 'Mountain'
	WHEN 'R' THEN 'Road'
	WHEN 'S' THEN 'Other Sales'
	WHEN 'T' THEN 'Touring'
	ELSE 'n/a'
END AS prd_line,
prd_start_dt,
LEAD(DATEADD(day, -1, prd_start_dt)) OVER (PARTITION BY prd_key ORDER BY prd_start_dt) AS prd_end_dt
FROM bronze.crm_prd_info

WHERE SUBSTRING(prd_key, 7, LEN(prd_key)) NOT IN -- Filter out unmatched data
(SELECT DISTINCT sls_prd_key FROM bronze.crm_sales_details WHERE sls_prd_key LIKE 'FK-16%')
-> 'FK-16%' is what known as pattern matching string. It is used in conjunction with the 'LIKE' operator, which is designed for flexible text searches in SQL.
-> 'FK-16' : this is a literal part of the string. It means the 'sls_prd_key' must start with the characters 'F','K','-','1','6'
-> '%' : this is a wildcard character

---

+ Write Transformation for Data & Filter for Duplicate
SELECT
cst_id,
cst_key,
TRIM(cst_firstname) AS cst_firstname,
TRIM(cst_lastname) AS cst_lastname,
cst_marital_status,
cst_gndr,
CASE WHEN cst_gndr = 'F' THEN 'Female'
          WHEN cst_gndr = 'M' THEN 'Male'
          ELSE 'n/a'
END cst_gndr,
cst_create_date
FROM (
SELECT
*,
ROW_NUMBER () OVER (PARTITION BY cst_id ORDER BY cst_create_date DESC) as flag_last
FROM bronze.crm_cust_info                   -> group, add new column
WHERE cst_id IS NOT NULL
)t WHERE flag_last = 1                            -> filter by column 
                                                                -> get no duplicate
                                                                -> then transform from it

+ Modification to the ddl (table definition) if neccessary once the transformation progress is done.

+ (FINAL) Re-run the quality check queries from the bronze layer to verify the quality of data in the silver layer after transformation, cleaning and then loading.

--Quality Checks
SELECT
prd_id,
COUNT(*)
FROM silver.crm_prd_info
GROUP BY prd_id
HAVING COUNT(*) >1 OR prd_id IS NULL

-- Check for unwanted Spaces
-- Expectation: No Results
SELECT prd_nm
FROM silver.crm_prd_info
WHERE prd_nm != TRIM(prd_nm)

-- Check for NULLS or Negative Numbers
-- Expectation: No Results
SELECT prd_cost
FROM silver.crm_prd_info
WHERE prd_cost < 0 OR prd_cost IS NULL

-- Data Standardization & Consistency
SELECT DISTINCT prd_line
FROM silver.crm_prd_info

-- Check for Invalid Date Orders
SELECT *
FROM silver.crm_prd_info
WHERE prd_end_dt < prd_start_dt

SELECT *
FROM silver.crm_prd_info


4. Docs & Version: Data Documenting Versioning in GIT
- Build Data Flow and Data Integration diagram

--> In the silver layer, sometimes we have to adjust the metadata if the quality of the data types and so on is not good or we are building new derived informations in order to later integrate the data

-- LOAD&CLEAN crm_sales_details
+ Check sls_prd_key and sls_cust_id
SELECT
sls_ord_num,
sls_prd_key,
sls_cust_id,
sls_order_dt,
sls_ship_dt,
sls_due_dt,
sls_sales,
sls_quantity,
sls_price
FROM bronze.crm_sales_details
WHERE sls_cust_id NOT IN (SELECT cst_id FROM silver.crm_cust_info)

+ Check sls_order_dt, sls_ship_dt, sls_due_dt
-> Firstly, negative numbers can't be cast to a date
SELECT
sls_order_dt
FROM bronze.crm_sales_details
WHERE sls_order_dt <= 0

-> Next, use NULLIF() : Returns NULL if two given values are equal; otherwise it returns the first expression
NULLIF(sls_order_dt,0) AS sls_order_dt

-> Examine the numbers
2010|12|29
 Year|Month|Day

-> In this scenario, the length of the date must be 8
...
WHERE sls_order_dt <= 8 OR LEN(sls_order_dt) != 8

-> Check for outliers by validating the boundaries of the date range
...
WHERE sls_order_dt > 20260101 OR sls_order_dt < 19950101

--> Combine all the conditions for validating
...
WHERE sls_order_dt <= 0
OR LEN(sls_order_dt) != 8
OR sls_order_dt > 20260101
OR sls_order_dt < 19950101

-> You can't cast Integer directly to Date in SQL Server, so first Integer -> VARCHAR(50) -> DATE
CASE WHEN sls_order_dt = 0 OR LEN(sls_order_dt) != 8 THEN NULL
          ELSE CAST(CAST(sls_order_dt AS VARCHAR) AS DATE)
END AS sls_order_dt

-> Order Date must always be earlier than the Shipping Date or Due Date

+ Check sls_sales, sls_quantity, sls_price
-> Business Rules: 
+ Total Sales = Quantity * Price
+ NO negative, zeros, nulls

SELECT
sls_sales,
sls_quantity,
sls_price
FROM bronze.crm_sales_details
WHERE sls_sales != sls_quantity * sls_price
OR sls_sales IS NULL
OR sls_quantity IS NULL
OR sls_price IS NULL
OR sls_sales <= 0
OR sls_quantity <= 0
OR sls_price <= 0
ORDER BY sls_sales, sls_quantity, sls_price

--> You have to talk with the experts in order to go forward with the best solution depending on their needs, different rules require different Transformations.

-> Rules: If Sales is negative, zero, or null, derive it using Quantity and Price. If Price is zero or null, calculate it using Sales and Quantity. If Price is negative, convert it to a positive value.
-> CASE WHEN

++ Quality Check of Silver Table
-- Check Data Consistency: Between Sales, Quantity, and Price
-- >> Sales = Quantity * Price
-- >> Values must not be NULL, zero, or negative.

SELECT DISTINCT
sls_sales,
sls_quantity,
sls_price
FROM silver.crm_sales_details
WHERE sls_sales != sls_quantity * sls_price
OR sls_sales IS NULL
OR sls_quantity IS NULL
OR sls_price IS NULL
OR sls_sales <= 0
OR sls_quantity <= 0
OR sls_price <= 0
ORDER BY sls_sales, sls_quantity, sls_price

-- Check invalid dates
SELECT * FROM silver.crm_sales_details
WHERE sls_order_dt > sls_ship_dt OR sls_order_dt > sls_due_dt

-- Final Check for everything
SELECT * FROM silver.crm_sales_details

-- LOAD&CLEAN erp_cust_az12
+ Check cid
-> After transformation, check if there are any mismatched data
SELECT
CASE WHEN cid LIKE 'NAS%' 
	THEN SUBSTRING(cid,4,LEN(cid))
	ELSE cid
END AS cid,
bdate,
gen
FROM bronze.erp_cust_az12
--WHERE cid LIKE '%AW00011000%'
WHERE CASE WHEN cid LIKE 'NAS%' 
	THEN SUBSTRING(cid,4,LEN(cid))
	ELSE cid 
END
	NOT IN 
(SELECT cst_key FROM silver.crm_cust_info)

+ Check bdate
-> Check for very old customers, and bdate in the future
SELECT DISTINCT
bdate
FROM bronze.erp_cust_az12
WHERE bdate > GETDATE() OR bdate < '1924-01-01'

++ Quality Check
-- Identify Out-of-Range Dates
SELECT DISTINCT
bdate
FROM silver.erp_cust_az12
WHERE bdate > GETDATE()

SELECT DISTINCT
gen,
CASE WHEN UPPER(TRIM(gen)) IN ('F', 'FEMALE') THEN 'Female'
	 WHEN UPPER(TRIM(gen)) IN ('M', 'MALE') THEN 'Male'
	ELSE 'n/a'
END AS gen
FROM silver.erp_cust_az12

SELECT DISTINCT cid FROM silver.erp_cust_az12

SELECT * FROM silver.erp_cust_az12 ORDER BY gen

-- LOAD&CLEAN erp_loc_a101
+ Check cid
REPLACE(cid, '-','') AS cid

+ Check cntry
CASE WHEN UPPER(TRIM(cntry)) IN ('DE','GERMANY') THEN 'Germany'
	 WHEN UPPER(TRIM(cntry)) IN ('USA','US','UNITED STATES') THEN 'United States'
	 WHEN UPPER(TRIM(cntry)) = '' OR cntry IS NULL THEN 'n/a'
	ELSE TRIM(cntry)
END AS cntry

+ Quality Check of Silver table
-- Data Standardization & Consistency
SELECT DISTINCT cntry
FROM silver.erp_loc_a101
ORDER BY cntry

SELECT * FROM silver.erp_loc_a101

-- LOAD&CLEAN erp_px_cat_g1v2
+ Check id
--> Everything is good to go

-- DOCUMENT: Data Flow

-----------
CREATE GOLD LAYER
-> Analysing: Explore & Understand the Business Objects
-> Coding: Data Integration 
 --> Build the Business Object
 --> Choose Type (Dimension vs Fact)
 --> Rename to friendly names
-> Validating: Data Integration Checks
-> Docs & Version: Documenting Versioning in GIT

- Theory: What is Data Modeling?
-> Taking raw and unorganized data from source to organize and structure them in a meaningful way (For example, put data into specific objects such as customers, products, sales then describe relationships between them) - e.g creating Logical Data Model

-> Three different stages: three different ways on how to draw a data model
 --> First stage: Conceptual Data Model
Focus only on the entity (Customers, Products, Orders) and don't go into details at all - e.g don't specify any columns or attributes inside these entities, only focus on the entities that we have as well as the relationship between them -> Give the BIG PICTURE.

 --> Second stage: Conceptual Data Model
Start specifying what are the different columns that we can find in each entity (For example, CustID First_Name Last_Name for Customers, OrderID CustID ProdID Order_Date Sales for Orders, Name Category for Products). As well as clearly marking the relationship between these  different columns from each entity, and which is the primary key respectively. -> More DETAILS, but still hasn't worried about how to store these tables in the database.

 --> Third stage: Physical Data Model
Where everything gets ready before creating it in the database like adding for each column the data types and the length of each data types and many other database techniques and details (e.g: CustID: Integer First_name: Varchar(20) Last_Name: Varchar(20) for Customers) -> IMPLEMENTATION

- Theory: Star Schema vs. Snowflake Schema
-> Optimized Data Model that is suitable for reporting and analyticsc and it should be flexible and scalable as well as easy to understand.

--> Star Schema: It has a central Fact table in the middle and surrounded by Dimension tables. Fac table contains transactional events and dimension tables contain descriptive information
--> Snowflake Schema: It has a central Fact table in the middle and surrounded by Dimension tables, similar to Star Schema. But the big difference is that we break the Dimensions into Subdimensions, e.g the surrounding dimension tables have relationships with its other dimension tables, creating a snowflake shape.

- Theory: Dimensions vs Facts
--> Dimension: Descriptive Information that give context to your data. (product info: product names, categories, subcategories)
-> Who? What? Where?
 
--> Fact: Quantitative information that represents events. (multiple products, dates, numbers - altogether in one table)
-> How much? How many?

BUILD GOLD LAYER
- Explore the Business Objects
a


